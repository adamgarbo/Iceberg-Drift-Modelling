#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 14 09:44:32 2021
Last modified: March 18, 2022

@author: Adam Garbo

calculate_error_metrics.py

Description:
    - Calculates error metrics for observed and modelled iceberg tracks,
    including distance, azimuth, speed.
    - Functions to plot the various calculated error metrics.

Requires:
    - Hindcast outputs generated by the NAIS iceberg drift model and
    observed iceberg tracking beacon data extracted from the iceberg
    tracking beacon database (temporary name).
    
Notes:
    - Python code formatted using Black:
    https://github.com/psf/black
    
"""

import os
import glob
import shutil
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import pyproj
from pyproj import Proj
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter


# -----------------------------------------------------------------------------
# Library Configuration
# -----------------------------------------------------------------------------

# Initialize pyproj with appropriate ellipsoid
geodesic = pyproj.Geod(ellps="WGS84")

# Add Natural Earth coastline
coast = cfeature.NaturalEarthFeature(
    "physical", "land", "10m", edgecolor="black", facecolor="lightgray", lw=0.75
)

# Add Natural Earth coastline
coastline = cfeature.NaturalEarthFeature(
    "physical", "coastline", "10m", edgecolor="black", facecolor="none", lw=0.75
)

# Configure Seaborn styles
sns.set_theme(style="ticks")
sns.set_context("talk")  # Options: talk, paper, poster
sns.set_style({"xtick.direction": "out", "ytick.direction": "out"})

# Configure
plt.rc("legend", fancybox=False, framealpha=1, edgecolor="k")

# Set colour palette
colour = ["red", "lime", "blue", "magenta", "cyan", "yellow"]
sns.set_palette(colour)

# Set figure DPI
dpi = 300

# -----------------------------------------------------------------------------
# Paths
# -----------------------------------------------------------------------------

# Path to figures
path_figures = "/Volumes/data/nais_iceberg_drift_model/output/validation/figures/error/"

# Path to data
path_data = "/Volumes/data/nais_iceberg_drift_model/"


# -----------------------------------------------------------------------------
# Statistics Functions
# -----------------------------------------------------------------------------

# Root mean square error (RMSE)
def rmse(values):
    return np.sqrt(sum(values ** 2) / len(values))


# Mean absolute error (MAE)
def mae(values):
    return np.average(np.abs(values))


# Standard deviation
def sd(values):
    return np.std(values)


# Coefficient of variation
def cv(values):
    return np.std(values) / np.mean(values) * 100


# Calculate statistics for Figures: 4.6, 4.10
def calculate_statistics(df):
    """
    

    Parameters
    ----------
    df : pandas DataFrame
        DESCRIPTION.

    Returns
    -------
    stats : TYPE
        DESCRIPTION.

    """
    
    
    # Create an empty dataframe
    stats = pd.DataFrame()
    stats = (
        df.groupby(["dur_obs", "current"])["dist_error"]
        .apply(mae)
        .reset_index(name="mae")
    )
    stats["sd"] = (
        df.groupby(["dur_obs", "current"])["dist_error"]
        .apply(sd)
        .reset_index()["dist_error"]
    )
    stats["cv"] = (
        df.groupby(["dur_obs", "current"])["dist_error"]
        .apply(cv)
        .reset_index()["dist_error"]
    )
    stats["rmse"] = (
        df.groupby(["dur_obs", "current"])["dist_error"]
        .apply(rmse)
        .reset_index()["dist_error"]
    )
    stats["scaled_error"] = (
        df.groupby(["dur_obs", "current"])["scaled_error"]
        .apply(rmse)
        .reset_index()["scaled_error"]
    )
    stats["count"] = (
        df.groupby(["dur_obs", "current"]).size().reset_index(name="count")["count"]
    )
    stats = stats.round({"mae": 2, "rmse": 2, "sd": 2, "cv": 2, "scaled_error": 2})
    
    # Optional: Output to CSV
    #stats.to_csv(path_data + "output/validation/statistics/stats.csv", index=False)

    return stats




# -----------------------------------------------------------------------------
# Calculate RMSE of entire iceberg tracks
# -----------------------------------------------------------------------------


# Subset statistics dataframe to look at specific time periods (e.g. daily)
stats_subset = stats[stats['dur_obs'].isin([24,48,72,96])]
pivoted_stats = stats.pivot('dur_obs', 'current').reset_index()
pivoted_stats.to_csv(path_data + "output/validation/statistics/pivoted_stats.csv" , index=False)


# Load data
df1 = load_data(2009)
df2 = load_data(2017)

# Calculate stats
calculate_statistics(df1)
calculate_statistics(df1)

# Create an empty dataframe
stats_track = pd.DataFrame()
stats_track = df.groupby(['ensemble','current'])['dist_error'].apply(rmse).reset_index(name='rmse')
# Pivot data
pivoted_stats_track = stats_track.pivot('ensemble', 'current').reset_index()
# Output to CSV
pivoted_stats_track.to_csv(path_data + "output/validation/statistics/pivoted_stats_track.csv" , index=False)


# -----------------------------------------------------------------------------
# Calculate error between modelled and observed iceberg trajectories
# Last confirmed working 2022-01-18
# -----------------------------------------------------------------------------

# Batch calculate error stastistics

# Path to modelled output
path_observed = path_data + "input/iceberg_database/interpolated/"
path_modelled = path_data + "output/validation/model/"
path_error = path_data + "output/validation/error/"

# Find all files in folder
files = glob.glob(path_modelled + "*.csv")

i = 0
# Iterate through each output file and calculate error metrics
for file in files:
    i += 1
    print("Processing %s (%s of %s)" % (file, i, len(files)))

    filename = os.path.splitext(os.path.basename(file))[0]

    # Calculate error
    calculate_error(filename, path_observed, path_modelled, path_error)

    # Debug: Iterate only once through loop
    break

def calculate_error(filename, path_observed, path_modelled, path_error):
    """
    
    Calculate distance error metrics betweeb observed and modelled iceberg tracks.

    Parameters
    ----------
    filename: str
        Path name to observed iceberg track file.
    path_observed: str
        Path name to observed iceberg track file.
    path_modelled: str
        Path name to modelled iceberg track file.
    path_error: str
        Path name to error iceberg track file.
    
    Returns
    -------
        None.

    """

    # -------------------------------------------------------------------------
    # Load data
    # -------------------------------------------------------------------------

    # Debug: Manually specify filename and paths to observed and modelled data
    # filename = '2019_1124-2670293_0'
    # path_observed = '/Volumes/data/nais_iceberg_drift_model/input/iceberg_database/interpolated/'
    # path_modelled = '/Volumes/data/nais_iceberg_drift_model/output/model/validation/'
    # path_error = '/Volumes/data/nais_iceberg_drift_model/output/error/'

    # Split beacon ID into year and IMEI
    year, beacon, interval = filename.split("_")

    # Load observed iceberg trajectory
    observed = pd.read_csv(
        path_observed + "%s_%s.csv" % (year, beacon),
        usecols=["datetime_data", "longitude", "latitude"],
        index_col=False,
    )

    # Load modelled trajectory
    modelled = pd.read_csv(
        path_modelled + filename + ".csv",
        usecols=[
            "id",
            "datetime",
            "branch",
            "longitude",
            "latitude",
            "length",
            "keel",
            "u_current",
            "v_current",
            "u_wind",
            "v_wind",
        ],
        index_col=False,
    )

    # -------------------------------------------------------------------------
    # Prepare data
    # -------------------------------------------------------------------------

    # Rename variables
    observed = observed.rename(
        columns={
            "datetime_data": "datetime",
            "longitude": "lon_obs",
            "latitude": "lat_obs",
        }
    )
    modelled = modelled.rename(
        columns={"longitude": "lon_model", "latitude": "lat_model"}
    )

    # Convert datetime strings to datetime64[ns] objects
    observed["datetime"] = pd.to_datetime(
        observed["datetime"].astype(str), format="%Y-%m-%d %H:%M:%S"
    )
    modelled["datetime"] = pd.to_datetime(
        modelled["datetime"].astype(str), format="%Y-%m-%d %H:%M:%S"
    )

    # Ensure observed datetime seconds are zeroed to avoid uncessary interpolation in Step 4
    # observed['datetime'] = observed['datetime'].dt.floor('min')

    # Trim modelled data based on date range of iceberg observations
    # Note:  It may be necessary for the model to wait for an initial time condition
    # to be met before it starts (e.g. GLORYS daily mean ocean current data)
    modelled = modelled[
        (modelled["datetime"] >= observed.datetime.min())
        & (modelled["datetime"] <= observed.datetime.max())
    ]

    # Trim observed data based on modelled time period in case the
    # observed track is longer in duration than the modelled period
    observed = observed[
        (observed["datetime"] >= modelled.datetime.min())
        & (observed["datetime"] <= modelled.datetime.max())
    ]

    # Check if time window is at least 24 hours in duration
    if (
        not (observed.datetime.max() - observed.datetime.min()).total_seconds() / 3600
        >= 24
    ):
        print("Warning: File %s is less than 24 hours in duration" % filename)
        return  # Skip this time window

    # -------------------------------------------------------------------------
    # Process observed data
    # -------------------------------------------------------------------------

    # Calculate forward azimuth and Great Circle distance between successive positions
    observed["dir_obs"], back_azimuth, observed["dist_obs"] = geodesic.inv(
        observed["lon_obs"].shift().tolist(),
        observed["lat_obs"].shift().tolist(),
        observed["lon_obs"].tolist(),
        observed["lat_obs"].tolist(),
    )

    # Convert azimuth from (-180° to 180°) to (0° to 360°)
    observed["dir_obs"] = (observed["dir_obs"] + 360) % 360

    # Convert distance to kilometres
    observed["dist_obs"] = observed["dist_obs"] / 1000.0

    # Calculate cumulative distance travelled
    observed["dist_csum_obs"] = observed["dist_obs"].cumsum()

    # Calculate speed
    # observed['speed_obs_cms'] = (observed['dist_obs'] * 100000) / observed['datetime'].diff().dt.seconds # Centimetre per second
    # observed['speed_obs_ms'] = (observed['dist_obs'] * 1000) / observed['datetime'].diff().dt.seconds # Metre per second
    observed["speed_obs"] = observed["dist_obs"] / (
        observed["datetime"].diff().dt.seconds / 3600.0
    )

    # Calculate interval between observations
    observed["int_obs"] = observed["datetime"].diff().dt.seconds / 3600

    # Calculate elapsed time (in hours)
    observed["dur_obs"] = (observed["datetime"].diff().dt.seconds / 3600.0).cumsum()

    # Calculate UTM zone of observed data based on first row value
    utm_zone = str((math.floor((observed["lon_obs"].iloc[0] + 180) / 6) % 60) + 1)

    # Reproject geographic coordinates to UTM
    pp = Proj(proj="utm", zone=utm_zone, ellps="WGS84", preserve_units=False)
    xx, yy = pp(observed["lon_obs"].values, observed["lat_obs"].values)
    observed["x_obs"] = xx
    observed["y_obs"] = yy

    # Subtract initial position values
    observed["x_obs"] = (observed["x_obs"] - observed["x_obs"].iloc[0]) / 1000
    observed["y_obs"] = (observed["y_obs"] - observed["y_obs"].iloc[0]) / 1000

    # Optional: Write processed observations to CSV file
    # observed.to_csv('/Volumes/data/nais_iceberg_drift_model/output/observed/%s.csv' % filename, index=False)

    # ------------------------------------------------------------------------
    # Merge observed and modelled data using common datetimes
    # -------------------------------------------------------------------------

    # Set datatime as index of dataframes for merge
    observed = observed.set_index("datetime")
    modelled = modelled.set_index("datetime")

    # Merge resulting interpolatd modelled dataframe with observations
    merged = modelled.merge(observed, left_index=True, right_index=True, how="inner")

    # Reset dataframe datetime indexes
    observed = observed.reset_index()
    modelled = modelled.reset_index()
    merged = merged.reset_index()

    # -------------------------------------------------------------------------
    # Calculate model error evaluation metrics
    # -------------------------------------------------------------------------

    # Create empty dataframe
    error = pd.DataFrame()

    # Iterate through model ouputs based on iceberg branch/id and perform
    # calculations using Pandas on a group-by-group basis
    for label, group in merged.groupby(["branch", "id"]):

        # Calculate forward azimuth and great circle distance between modelled coordinates
        group["dir_model"], back_azimuth, group["dist_model"] = geodesic.inv(
            group["lon_model"].shift().tolist(),
            group["lat_model"].shift().tolist(),
            group["lon_model"].tolist(),
            group["lat_model"].tolist(),
        )

        # Convert distance from metres to kilometres
        group["dist_model"] = group["dist_model"] / 1000.0

        # Calculate cumulative distance traveled
        group["dist_csum_model"] = group["dist_model"].cumsum()

        # Calculate distance error between observed and modelled coordinates
        fwd_azimuth, back_azimith, group["dist_error"] = geodesic.inv(
            group["lon_obs"].tolist(),
            group["lat_obs"].tolist(),
            group["lon_model"].tolist(),
            group["lat_model"].tolist(),
        )

        # Convert error distance error from metres to kilometres
        group["dist_error"] = group["dist_error"] / 1000.0

        # Convert azimuth from (-180° to 180°) to (0° to 360°)
        group["dir_model"] = (group["dir_model"] + 360) % 360

        # Calculate direction error
        group["dir_error"] = group["dir_obs"] - group["dir_model"]

        # Calculate cumulative distance error between observed and modelled coordinates
        group["dist_csum_error"] = group["dist_csum_obs"] - group["dist_csum_model"]

        # Calculate speed
        group["speed_model"] = group["dist_model"] / (
            group["datetime"].diff().dt.seconds / 3600.0
        )

        # Calculate scaled error (distance error / cumulative distance traveled)
        group["scaled_error"] = group["dist_error"] / group["dist_csum_obs"]

        # Calculate speed error
        group["speed_error"] = group["speed_obs"] - group["speed_model"]

        # ---------------------------------------------------------------------
        # Reproject geographic coordinates to UTM
        # ---------------------------------------------------------------------

        pp = Proj(proj="utm", zone=utm_zone, ellps="WGS84", preserve_units=False)
        xx, yy = pp(group["lon_model"].values, group["lat_model"].values)
        group["x_model"] = xx
        group["y_model"] = yy

        # Subtract initial position values
        group["x_model"] = (group["x_model"] - group["x_model"].iloc[0]) / 1000
        group["y_model"] = (group["y_model"] - group["y_model"].iloc[0]) / 1000

        # Concatenate branch and id columns for plotting by group
        group["ensemble"] = str(label[1]) + "_" + str(label[0])

        # Append calculated data into new dataframe
        error = error.append(group, ignore_index=False)

        # Sory by index (optional)
        error = error.sort_index()

        # Round dataframe columns
        error = error.round(
            {
                "lat_model": 6,
                "lon_model": 6,
                "lat_obs": 6,
                "lon_obs": 6,
                "dir_obs": 2,
                "dist_obs": 4,
                "dist_csum_obs": 4,
                "speed_obs": 2,
                "x_obs": 6,
                "y_obs": 6,
                "dir_model": 2,
                "dist_model": 4,
                "dist_csum_model": 4,
                "dist_error": 4,
                "dir_error": 2,
                "dist_csum_error": 4,
                "speed_model": 4,
                "scaled_error": 4,
                "speed_error": 2,
                "x_model": 6,
                "y_model": 6,
            }
        )

    # Write error evaluation metrics to CSV file
    try:
        error.to_csv(path_error + "%s.csv" % filename, index=False)

    except ValueError:
        print("Warning: Unable to write %s to CSV" % filename)


# ----------------------------------------------------------------------------
# Merge calculated error outputs into a single CSV file
# ----------------------------------------------------------------------------

# 2009-2019
path_input = path_data + "output/error/2009-2019/"
path_output = path_data + "output/merged/error.csv"

# 2017-2019
path_input = path_data + "output/error/2017-2019/"
path_output = path_data + "output/merged/error_2017.csv"

# Identify all files contained in the folder
files = sorted(glob.glob(path_input + "*.csv"))

# Concatenate CSV files
with open(path_output, "w") as outfile:
    for i, file in enumerate(files):
        with open(file, "r") as infile:
            if i != 0:
                infile.readline()  # Throw away header on all but first file
            # Block copy rest of file from input to output without parsing
            shutil.copyfileobj(infile, outfile)
            print(file + " has been imported.")
            
            
# -----------------------------------------------------------------------------
# Load model error outputs 
# -----------------------------------------------------------------------------

def load_data(year):
    """


    Parameters
    ----------
    year : int
        Starting year of hindcast period (2009 or 2017).

    Returns
    -------
    df : pandas DataFrame
        Returns a pandas DataFrame of the selected hindcast period.

    """

    # Read CSV
    df = pd.read_csv(
        path_data + "output/validation/merged/error_%s.csv" % year,
        index_col=False,
    )

    # Drop nan rows
    df = df.dropna()

    # Convert to datetime
    df["datetime"] = pd.to_datetime(
        df["datetime"].astype(str), format="%Y-%m-%d %H:%M:%S"
    )

    # Add duration column
    df["dur_obs"] = df["dur_obs"].astype(int)

    # Add ocean current model column
    df["current"] = df["branch"].str.split("_").str[1]

    if year == 2009:
        df = df[df["current"] != "riops"]  # Drop RIOPS

    return df


            
# -----------------------------------------------------------------------------
# Plot map of modelled and observed iceberg trajectories
# Note: Not used in thesis.
# -----------------------------------------------------------------------------

# Specify path to model outputs as either:
path_input = path_data + "output/validation/error/2009-2016/"
# or:
path_input = path_data + "output/validation/error/2017-2019/"

# Specify path to output figures
path_figures = path_data + "output/validation/figures/error_metrics/"

# Find all files in folder
files = glob.glob(path_input + "*.csv")

# Iterate through each error file and produce figures
for file in files:

    # Extract filename from path
    filename = os.path.splitext(os.path.basename(file))[0]

    # Produce figures
    plot_error(filename, path_input, path_figures)

    # Debug: Iterate only once through loop
    break

def plot_map(beacon_id, error):
    """

    Parameters
    ----------
    error : TYPE
        DESCRIPTION.

    Returns
    -------
    error : TYPE
        DESCRIPTION.

    """

    # Split filaname
    year, beacon, interval = beacon_id.split("_")

    # Set path to modelled data
    path_modelled = "/Volumes/data/nais_iceberg_drift_model/output/figures/"

    # Adjust labels according to number of unique environmental input data sources
    if error["branch"].nunique() == 2:
        label1 = ["Observed", "CECOM", "GLORYS"]
    elif error["branch"].nunique() == 3:
        label1 = ["Observed", "CECOM", "GLORYS", "RIOPS"]

    x = error["lat_obs"].median()
    y = error["lon_obs"].median()

    plt.figure(figsize=(10, 10))
    ax = plt.axes(projection=ccrs.PlateCarree())
    ax.add_feature(coast)
    # ax.set_xmargin(0.10) # Uncomment to increase padding around line plots
    # ax.set_ymargin(0.10)
    ax.set_adjustable("datalim")
    lon_formatter = LongitudeFormatter(number_format=".1f")
    lat_formatter = LatitudeFormatter(number_format=".1f")
    gl = ax.gridlines(
        crs=ccrs.PlateCarree(),
        draw_labels=True,
        color="black",
        alpha=0.3,
        linestyle="dotted",
        xformatter=lon_formatter,
        yformatter=lat_formatter,
    )
    gl.top_labels = False
    gl.right_labels = False
    gl.rotate_labels = False
    sns.lineplot(
        x="lon_obs",
        y="lat_obs",
        color="blue",
        data=error,
        label="Observed",
        ci=None,
        sort=False,
        transform=ccrs.PlateCarree(),
    )
    sns.lineplot(
        x="lon_model",
        y="lat_model",
        hue="ensemble",
        data=error,
        ci=None,
        sort=False,
        transform=ccrs.PlateCarree(),
    )
    ax.set_title(
        beacon_id
        + " "
        + str(error["datetime"].min())
        + " - "
        + str(error["datetime"].max()),
        loc="left",
        fontweight="bold",
    )
    ax.legend(labels=label1)

    # Save figure
    plt.savefig(
        path_modelled + "00_map_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    
    # Close plot
    plt.close()


# -----------------------------------------------------------------------------
# Batch lot figures of error metrics
# Last confirmed working 2022-01-18
# -----------------------------------------------------------------------------

# Specify path to model outputs as either:
path_input = path_data + "output/validation/error/2009-2016/"
# or:
path_input = path_data + "output/validation/error/2017-2019/"

# Specify path to output figures
path_figures = path_data + "output/validation/figures/error_metrics/"

# Find all files in folder
files = glob.glob(path_input + "*.csv")

# Iterate through each error file and produce figures
for file in files:

    filename = os.path.splitext(os.path.basename(file))[0]

    # Plot error
    plot_error(filename, path_input, path_figures)

    # Debug: Iterate only once through loop
    break


def plot_error(filename, path_input, path_output):
    """

    Produce figures of calculated error metrics.
    
    Parameters
    ----------
    filename : str
        Name of modelled output error file.
    path_input : str
        Path to modelled output error files.
    path_output : str
        Path to folder to save figures.

    Returns
    -------
    None.

    """

    # Interval between x-axis ticks
    tick_spacing = 12 
    
    # Split filename
    year, beacon, interval = filename.split("_")
    beacon_id = "%s_%s_%s" % (year, beacon, interval)

    # Debug: Set paths to observed and modelled data
    # path_input = '/Volumes/data/nais_iceberg_drift_model/output/error/'

    # Debu: Set paths to figures
    # path_figures = '/Volumes/data/nais_iceberg_drift_model/output/figures/'

    # Load observed iceberg trajectory
    df = pd.read_csv(path_input + filename + ".csv", index_col=False)

    # Change column to string
    df["branch"] = df["branch"].astype(str)

    # Set colour palette
    # sns.set_palette('turbo', 3)

    # Adjust labels according to number of unique environmental input data sources
    if df["branch"].nunique() == 1:
        label1 = ["Observed", "RIOPS"]
        label2 = ["RIOPS"]
    if df["branch"].nunique() == 2:
        label1 = ["Observed", "CECOM", "GLORYS"]
        label2 = ["CECOM", "GLORYS"]
    elif df["branch"].nunique() == 3:
        label1 = ["Observed", "CECOM", "GLORYS", "RIOPS"]
        label2 = ["CECOM", "GLORYS", "RIOPS"]

    # -------------------------------------------------------------------------
    # Map (distance)
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.grid(ls="dotted")
    ax.set_adjustable("datalim")
    ax.set_aspect("equal")
    ax.set(xlabel="East (km)", ylabel="North (km)")
    # ax.set_title(str(df['datetime'].min()) + ' - ' +  str(df['datetime'].max()), loc='left', fontweight='bold') # filename + ' ' +
    sns.lineplot(
        x="x_obs",
        y="y_obs",
        color="black",
        data=df,
        label="Observed",
        ci=None,
        sort=False,
    )
    sns.lineplot(
        x="x_model",
        y="y_model",
        data=df,
        hue="branch",
        style="branch",
        markers=True,
        markevery=3,
        dashes=False,
        ci=None,
        sort=False,
    )
    ax.legend(labels=label1)
    fig.savefig(
        path_figures + "01_map_%s.png" % filename,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Distance error
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Hindcast duration (hours)", ylabel="Distance error (km)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(x="dur_obs", y="dist_error", hue="ensemble", data=df, ci=None)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), labels=label2)
    sns.despine()
    fig.savefig(
        path_figures + "02_distance_error_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Scaled distance error
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Hindcast duration (hours)", ylabel="Scaled distance error")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(
        x="dur_obs", y="scaled_error", hue="ensemble", data=df, markers=True, ci=None
    )
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), labels=label2)
    sns.despine()
    fig.savefig(
        path_figures + "03_scaled_error_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Cumulative Distance
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Hindcast duration (hours)", ylabel="Cumulative distance (km)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(
        x="dur_obs",
        y="dist_csum_obs",
        data=df,
        color="black",
        label="Observed",
        ci=None,
    )
    sns.lineplot(x="dur_obs", y="dist_csum_model", data=df, hue="ensemble", ci=None)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), labels=label1)
    sns.despine()
    fig.savefig(
        path_figures + "04_distance_csum_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Speed
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Hindcast duration (hours)", ylabel="Speed (km/h)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(
        x="dur_obs", y="speed_obs", data=df, color="black", label="Observed", ci=None
    )
    sns.lineplot(x="dur_obs", y="speed_model", hue="ensemble", data=df, ci=None)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5),labels=label1)
    sns.despine()
    fig.savefig(
        path_figures + "05_speed_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Speed error
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Hindcast duration (hours)", ylabel="Speed error (km/h)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(x="dur_obs", y="speed_error", data=df, hue="ensemble", ci=None)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), labels=label2)
    sns.despine()
    fig.savefig(
        path_figures + "06_speed_error_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()

    # -------------------------------------------------------------------------
    # Direction
    # -------------------------------------------------------------------------

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Duration (hours)", ylabel="Direction (°)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(
        x="dur_obs", y="dir_obs", data=df, label="Observed", color="black", ci=None
    )
    sns.lineplot(x="dur_obs", y="dir_model", data=df, hue="ensemble", ci=None)
    ax.legend(loc="center left", bbox_to_anchor=(1.0, 0.5), labels=label1)
    sns.despine()
    fig.savefig(
        path_figures + "07_direction_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )

    # -------------------------------------------------------------------------
    # Drift Angle Error
    # -------------------------------------------------------------------------

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.set(xlabel="Duration (hours)", ylabel="Direction error (°)")
    ax.grid(ls="dotted")
    ax.xaxis.set_major_locator(mticker.MultipleLocator(tick_spacing))
    sns.lineplot(x="dur_obs", y="dir_error", data=df, hue="ensemble", ci=None)
    ax.legend(loc="center left", bbox_to_anchor=(1.0, 0.5), labels=label2)
    sns.despine()
    fig.savefig(
        path_figures + "08_drift_angle_error_%s.png" % beacon_id,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()


# ----------------------------------------------------------------------------
# Batch plot model output maps using a distance grid instead of lat/lon
# Note: These plots are contained in Appendix B
# Last confirmed working 2022-01-18 
# ----------------------------------------------------------------------------

# Specify path to model outputs as either:
path_input = path_data + "output/validation/error/2009-2016/"
# or:
path_input = path_data + "output/validation/error/2017-2019/"

# Specify path to output figures
path_figures = path_data + "output/validation/figures/appendix/"

# Find all files in folder
files = glob.glob(path_input + "*.csv")

# Iterate through each error file and create map
for file in files:

    fname = os.path.splitext(os.path.basename(file))[0]

    # Plot error
    plot_distance_maps(fname, path_input, path_figures)

    # Debug
    break


def plot_distance_maps(filename, path_input, path_figures):
    """

    Produce map of modelled and observed iceberg trajectories as distance
    measurements instead of latitude/longitude coordiantes. 
    Distances are specific to relative UTM zone.
    
    Parameters
    ----------
    filename : str
        Filename of model ouput.
    path_input : str
        Path to model outputs.
    path_figures : str
        Path to output figures.

    Returns
    -------
    None.

    """

    # Split filename
    year, beacon, interval = filename.split("_")
    beacon_id = "%s_%s_%s" % (year, beacon, interval)

    # Load observed iceberg trajectory
    df = pd.read_csv(path_input + filename + ".csv", index_col=False)

    # Change column to string
    df["branch"] = df["branch"].astype(str)

    # Adjust labels according to number of unique environmental input data sources
    if df["branch"].nunique() == 1:
        label1 = ["Observed", "RIOPS"]
        label2 = ["RIOPS"]
    if df["branch"].nunique() == 2:
        label1 = ["Observed", "CECOM", "GLORYS"]
        label2 = ["CECOM", "GLORYS"]
    elif df["branch"].nunique() == 3:
        label1 = ["Observed", "CECOM", "GLORYS", "RIOPS"]
        label2 = ["CECOM", "GLORYS", "RIOPS"]


    # Plot
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.grid(ls="dotted")
    ax.set_adjustable("datalim")
    ax.set_aspect("equal")
    ax.set(xlabel="East (km)", ylabel="North (km)")
    # ax.set_title(str(df['datetime'].min()) + ' - ' +  str(df['datetime'].max()), loc='left', fontweight='bold') # filename + ' ' +
    ax.set_title(
        "%s-%s %s - %s"
        % (beacon, interval, df["datetime"].min(), df["datetime"].max()),
        loc="left",
        fontweight="bold",
    )
    sns.lineplot(
        x="x_obs",
        y="y_obs",
        data=df,
        color="black",
        label="Observed",
        ci=None,
        sort=False,
    )
    sns.lineplot(
        x="x_model",
        y="y_model",
        data=df,
        hue="branch",
        style="branch",
        dashes=[(5, 1), (1, 1), (3, 1, 1, 1)],
        ci=None,
        sort=False,
    )
    ax.legend(labels=label1)
    fig.savefig(
        path_figures + "map_%s.png" % filename,
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    # plt.close()


# ----------------------------------------------------------------------------
# Batch plot maps of hindcast simulations of iceberg drift on a regular 
# latitude/longitude grid
# Last confirmed working 2022-01-18
# ----------------------------------------------------------------------------

# Specify path to model outputs as either:
path_input = path_data + "output/validation/error/2009-2016/"
# or:
path_input = path_data + "output/validation/error/2017-2019/"

# Specify path to output figures
path_figures = path_data + "output/validation/figures/appendix/"

# Find all files in folder
files = glob.glob(path_input + "*.csv")

# Iterate through each error file and create map
for file in files:

    fname = os.path.splitext(os.path.basename(file))[0]

    # Plot modelled hindcasts 
    plot_hindcast_maps(fname, path_input, path_figures)

    # Debug: Iterate through loop only once
    break


def plot_hindcast_maps(filename, path_input, path_figures):
    """

    Produce map of modelled and observed iceberg trajectories using a regular
    latitude/longitude grid and with coastal shapefile. 
    Note: Cartopy's orthographic map projection is used.

    Parameters
    ----------
    filename : str
        Filename of model ouput.
    path_input : str
        Path to model outputs.
    path_figures : str
        Path to output figures.

    Returns
    -------
    None.

    """

    # Split filename
    year, beacon, interval = filename.split("_")

    # Load observed iceberg trajectory
    df = pd.read_csv(path_input + filename + ".csv", index_col=False)

    # Change column to string
    df["branch"] = df["branch"].astype(str)

    # Adjust labels according to number of unique environmental input data sources
    if df["branch"].nunique() == 2:
        labels = ["Observed", "CECOM", "GLORYS"]
        dashes = [(5, 1), (1, 1)]
    elif df["branch"].nunique() == 3:
        labels = ["Observed", "CECOM", "GLORYS", "RIOPS"]
        dashes = [(5, 1), (1, 1), (3, 1, 1, 1)]

    plt.figure(figsize=(10, 10))
    ax = plt.axes(
        projection=ccrs.Orthographic(df["lon_obs"].median(), df["lat_obs"].median())
    )
    ax.add_feature(coast)
    ax.set_adjustable("datalim")
    lon_formatter = LongitudeFormatter(number_format=".1f")
    lat_formatter = LatitudeFormatter(number_format=".1f")
    gl = ax.gridlines(
        crs=ccrs.PlateCarree(),
        draw_labels=True,
        color="black",
        alpha=0.3,
        linestyle="dotted",
        xformatter=lon_formatter,
        yformatter=lat_formatter,
    )
    gl.top_labels = False
    gl.right_labels = False
    gl.rotate_labels = False
    sns.lineplot(
        x="lon_obs",
        y="lat_obs",
        color="black",
        data=df,
        label="Observed",
        ci=None,
        sort=False,
        transform=ccrs.PlateCarree(),
    )
    sns.lineplot(
        x="lon_model",
        y="lat_model",
        hue="ensemble",
        style="branch",
        dashes=dashes,
        data=df,
        ci=None,
        sort=False,
        transform=ccrs.PlateCarree(),
    )
    ax.set_title(
        "%s-%s %s - %s"
        % (beacon, interval, df["datetime"].min(), df["datetime"].max()),
        loc="left",
        fontweight="bold",
    )
    ax.legend(labels=labels)

    # Save figure
    plt.savefig(
        path_figures + "%s_%s_%s.png" % (year, beacon, interval),
        dpi=dpi,
        transparent=False,
        bbox_inches="tight",
    )
    plt.close()
